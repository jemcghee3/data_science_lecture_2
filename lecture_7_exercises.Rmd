---
title: "Decision Trees"
output: html_notebook
---

# Exercise 1 - Explore the Hiters data

```{r}
library(tree)
library(ISLR)
library(ggplot2)
```
get rid of old stuff
```{r}
rm(list = ls())
par(mfrow = c(1,1)) # set plotting window to default
```

# Regression trees

```{r}
data("Hitters") # In the tree package
?Hitters
head(Hitters)
```

Remove incomplete observations


```{r}
sum(is.na(Hitters))
Hitters <- na.omit(Hitters)
```
Salary is our target variable
We log-transfoorm it so that its distribution has more of a typical bell-shape.
We need to remember later on that Salary is logarithmized!
```{r}
hist(Hitters$Salary)
Hitters$Salary <- log(Hitters$Salary)
hist(Hitters$Salary)
```

```{r}
attach(Hitters)
```

# Exercise 2 - Fit a regression tree on the Hitters data (2 input variables)

Sample 70% of the row indices for subsetting as training data.
```{r}
set.seed(1)
trainHit <- sample(1:nrow(Hitters), 0.7*nrow(Hitters))
Hitters.train <- Hitters[trainHit,]
Hitters.test <- Hitters[-trainHit,]
```

Fir a regression tree to predict Salary from Years and Hits
```{r}
tree.salaryHitters <- tree(Salary ~ Years + Hits, data = Hitters)
```

tree() uses binary recursive partitioning.
The split which maximizes the reduction in impurity is chosen.
The data set is then split and the process is repeated.
Splitting continues until the terminal nodes are too small or too few to be split.
Tree growth is limited to a depth of 31 by the use of integers to label nodes.

```{r}
summary(tree.salaryHitters)
```

Output:
-  There are 8 terminal nodes ("leaves") of the tree.
-  Here "residual mean deviance" is just mean squared error (RMS)

Plot the regression tree
```{r}
plot(tree.salaryHitters)
text(tree.salaryHitters, cex = 0.75) # cex sets the character size
```

Plot the corresponding regions.

simple plot
```{r}
plot(Hitters$Years, Hitters$Hits, col = 'steelblue', pch = 20, xlab = "Years", ylab = "Hits")
partition.tree(tree.salaryHitters, ordvars = c("Years", "Hits"), add = TRUE, cex = 1)
```

Plot with salary value in color code
Prepare Salary data for plot
```{r}
salary.deciles = quantile(Salary, 0:10/10)
cut.salary = cut(Hitters$Salary, salary.deciles, include.lowest = TRUE)
```

Plot the point cloud and regions

```{r}
plot(Years, Hits, col = grey(10:2/11)[cut.salary], pch = 20, xlab = "Years", ylab = "Hits")
partition.tree(tree.salaryHitters, ordvars = c("Years", "Hits"), add = TRUE, cex = 1)

```

```{r}

tree.salaryHitters
```
node): node number
split: split criterion, e.g. Thal: normal, or Ca < 0.5
n: number of observations in that branch
deviance (the smaller the better)
yval: overall prediction for th ebranch (mean value Yes or No)
(ybrob): the fraction of observations in that branch that take on values of (Yes No)
* denotes terminal node


## Fit a regression tree on the Hitters data (all input variables)

```{r}
tree.salaryHitters <- tree(Salary ~ .-Salary, data = Hitters)
summary(tree.salaryHitters)
```

Plot the regression tree
```{r}
plot(tree.salaryHitters)
text(tree.salaryHitters, cex = 0.75)
```

Use the tree to make predictions on the test set
```{r}
tree.salaryHitters.pred <- predict(tree.salaryHitters, newdata = Hitters.test)
```

Compare predictions of the regression tree with true values (visually)
We plot the predictions against ground truth.
A perfect prediction would give a line with intercept 0 and slope 1
```{r}
salaryHitters.test <- Hitters.test$Salary
plot(tree.salaryHitters.pred,salaryHitters.test)
abline (0,1)
```

Calculate the mean squared error on test data
```{r}
salaryHitters.test <- Hitters[-trainHit ,"Salary"]
(tree.salaryHitters.MSE <- mean((tree.salaryHitters.pred - salaryHitters.test)^2))
sqrt(tree.salaryHitters.MSE)
```

MSE = 0.189
Square root of MSE = 0.383
Test predictions are within around $383 of the true median logarithmized salary.

# Exercise 3 - Pruning the regression tree (Hitters data)

Cost-Complexity Pruning
Goal: prune the trees to avoid high variance and overfitting.
Positive effects:
-  smaller *test" errors (due to less overfitting)
-  higher interpretability (due to smaller trees)

Use cross-validation to find the optimal parameter / alpha for cost-complexity pruning
```{r}
set.seed(3)
?cv.tree
cv.Hitters = cv.tree(tree.salaryHitters)
```

Runs a K-fold cross validation experiment to find the number of misclassifications as a function of the cost-complexity parameter /alpha


```{r}
cv.Hitters
```

$k: cost-complexity parameter (corresponds to /alpha)
Notice that /alpha is increasing (corresponding tot he pruning sequence)
$size: number of terminal nodes of each tree
Notice that the size is decreasing (corresponding to the pruning sequence)
$dev: *cross validation* error rate
The full tree (with size 9, i.e. 9 terminal nodes) has the lowest cross-validation error

Plot the cross-validation error-rate as a function of both size and /alpha (k):
```{r}
par(mfrow=c(1,2))
plot(cv.Hitters$size, cv.Hitters$dev, type = "b") # type = "b= means plot both, points and lines
plot(cv.Hitters$k, cv.Hitters$dev, type = "b")
par(mfrow=c(1,1))
```

We do not need to prune the tree, since the full tree has minimal cross-validation error
Yet, to show how to prune, here is the code
```{r}
prune.salaryHitters <- prune.tree(tree.salaryHitters, best = 6)
```

prune.tree determines the nested cost-complexity sequence
best = 6 gets the 6-node tree in the cost-complexity sequence

Plot the pruned regression tree
```{r}
plot(prune.salaryHitters)
text(prune.salaryHitters, cex = 0.75)
```

# Exercise 4 - Fitting and pruning a classification tree (Heart data)

Classification trees

load and inspect the Heart data set
This data contains a binary outcome HD for 303 patients who presented with chest pain.
An outcome value of Yes indicates the presence of heart disease based on an angiographic test,
while No means no heart disears. There are 13 predictors including Age, Sex, Chol (a cholestorol measurement) and other heart and lung function measurements

```{r}
Heart <- read.csv("./Heart.csv")
attach(Heart)
head(Heart)
Heart <- Heart [,-1] # Remove the row identifier (we don't use it as a predictor)
```

The categorical varaibles need to be of type factor rather than character to work with tree()
```{r}
str(Heart)
Heart$AHD = as.factor(Heart$AHD)
Heart$ChestPain = as.factor(Heart$ChestPain)
Heart$Thal = as.factor(Heart$Thal)
plot(Heart$AHD)
```

Sample 70% of the row indixes for subsetting as training data

```{r}
set.seed(2)
trainHeart <- sample(1:nrow(Heart), 0.7*nrow(Heart))
Heart.train <- Heart[trainHeart,]
Heart.test <- Heart[-trainHeart,]

```

Train classification tree on training data
```{r}
tree.AHDHeart <- tree(AHD ~ .-AHD, data =Heart.train)
```

```{r}
plot(tree.AHDHeart)
text(tree.AHDHeart, cex = 0.75, pretty = 0)
```

pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
Ca is the most important indicator of heart disease.

```{r}
summary(tree.AHDHeart)
```

Deviance:
For classification trees this is a scaled version of the entropy
Measured as -2 * (sum_{m} sum{k} n_{mk} * log(p_{mk}))
Here, n_{mk} is the number of observatiosn in the mth terminal node that belong to the kth class.
A small deviance indicates a tree that provides a good fit to the training data
Residual mean deviance:
Deviance divided by (n - IT_{0})
Misclassification error rate:
training error rate is 8.5%


```{r}
tree.AHDHeart
```

node): node number
split: split criterion e.g., Thal: normal or Ca < 0.5
n: number of observations in that branch
deviance (the smaller the better)
yval: overall prediction for the branch (Yes or No)
(ybrob): the fraction of observations in that branch that take on the values (Yes No)
* denotes terminal node

use classification trees to predict *test data*
```{r}
tree.AHDHeart.pred <- predict(tree.AHDHeart, Heart.test, type = "class")
```

Confusion table to determine classification error on test data
```{r}
(tree.AHDHeart.pred.ct <- table(tree.AHDHeart.pred, Heart.test$AHD))
(tree.AHDHeart.pred.correct <- (tree.AHDHeart.pred.ct[1,1] + tree.AHDHeart.pred.ct[2,2]) / sum(tree.AHDHeart.pred.ct)) # portion incorrectly classified
(tree.AHD.testError <- 1 - tree.AHDHeart.pred.correct) # test error
```

## Cost-complexity pruning
Goal: prune trees to avoid high variance and overfitting
Positive effects: 
-  smaller *test* errors (due to less overfitting)
-  higher interpretability (due to smaller trees)

use cross-validation to find the optimal parameter alpha for cost-complexity pruning
```{r}
set.seed(3)
cv.Heart = cv.tree(tree.AHDHeart, FUN = prune.misclass)
```

Runs a k-fold cross validation experiment to find the number of misclassifications as a function of the cost-complexity parameter alpha.
"Fun = prune.misclass":
  The *classification error rate* should guide the cross-validation and pruning process (as opposed to Gini index or entropy)
  If FUN is not specified, deviance is used as default (which is a version of entropy)
  --> Remember: If prediction accuracy is the goal, the error rate is preferable for pruning.
  

```{r}
cv.Heart
```

$k: cost-complexity parameter (corresponds to alpha)
Notice that /alpha is increasing (corresponding tot he pruning sequence)
alpha = 0.75 gives the lowest cross-validation error
$size: number of terminal nodes of each tree
Notice that the size is decreasing (corresponding to the pruning sequence)
$dev: *cross validation* error rate
The full tree (with size 8, i.e. 8 terminal nodes) has the lowest cross-validation error

plot the cross-validation error-rate as a function of both size and alpha
```{r}
par(mfrow=c(1,2))
plot(cv.Heart$size, cv.Heart$dev, type = "b")
plot(cv.Heart$k, cv.Heart$dev, type = "b")
par(mfrow=c(1,1))
```

do the actual pruning
```{r}
prune.AHDHeart <- prune.misclass(tree.AHDHeart, best = 7)

```

prune.misclass:
  is an abbreviation for prune.tree(method = "misclass") for use with cv.tree
  Here, prune.tree determines the nested cost-complexity sequence
best = 7: get hte 7 node tree in the cost-complexity sequence


plot the pruned tree
```{r}
plot(prune.AHDHeart)
text(prune.AHDHeart, pretty = 0)
```

use the pruned tree to predict test data
```{r}
prune.AHDHeart.pred <- predict(prune.AHDHeart, Heart.test, type = "class")

```

confusion table to determine classification error on test data
```{r}
(prune.AHDHeart.pred.ct <- table(prune.AHDHeart.pred, Heart.test$AHD))
(prune.AHDHeart.correct <- (prune.AHDHeart.pred.ct[1,1] + prune.AHDHeart.pred.ct[2,2]) / sum(prune.AHDHeart.pred.ct))
(prune.AHDHeart.testError <- 1 - prune.AHDHeart.correct)
```

Compare with test error of unpruned tree
```{r}
tree.AHD.testError
```
Smaller error with pruning!

# Exercise 5 - Bagged Trees

We apply bagging and random forest on Boston data set using the randomForest library in R.
The exact results may depend on the version of R and of randomForest
Recall that bagging is just a special version of random forest with m = p.

```{r}
library(MASS)
library(randomForest)
```

We will use the Boston dataset in the MASS library, which has housing values

Setup a training seed and test set for the Boston data
```{r}
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
boston.test = Boston[-train,"medv"]
```

Apply bagging using randomForest package in R
```{r}
bag.boston = randomForest(medv~., data=Boston, subset = train, mtry=13, importance = TRUE)
```
mtry = 13, meaning we should use all 13 predictors for each split of the tree
hence, do bagging

How well does the bagged model perform on the test set?
```{r}
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```
The test MSE associated with the bagged regression tree is 23.46
That is almost half that obtained using an optimally pruned single tree
(investigate later)

Exercise: Change the number of trees grown by randomForest() using the ntree argument
For example, what happens when we go from 13 to 25?

# Exercise 6 - Random Forest
Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument.
By default, randomForest uses p/3 variables when building a random forest of regression trees and sqrt(p) variables when building a random forest of classification trees.

Building a random forest on the same data set using mtry = 6
Comment on the difference from the test MSE from using the random forest compared to bagging.

```{r}
set.seed(1)
rf.boston=randomForest(medv~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf = predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf-boston.test)^2)
```
We see that the MSE for the random forest is 19.62, which is 25% lower than bagging

Investigate variable importance
```{r}
importance(rf.boston)
```
Two measures of variable importance are reported:
1.  The first based on the mean *decrease in accuracy* in predictions on the out of bag samples when a given variable is excluded from the model.
2.  The second is a measure of the total *decrease in node impurity* that results from splits over that variable, averaged over all trees

```{r}
varImpPlot(rf.boston)
```
The results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variabls for median house price (which makes sense)

# Exercise 7 - Boosted Tree

```{r}
library(gbm)
```
We use the gbm package and within it the gbm() function to fit boosted regression trees to the Boston data set.

Perform boosting on the training data set, treating this as a regression problem.

```{r}
set.seed(1)
boost.boston = gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
```
We run gbm() with the option distribution = "gaussian" since this is a regression problem.
if it were a binary classification problem, we would use distribution = "bernoulli"
"interaction.depth" refers to the maximum depth of variable interactions.
1 implies an additive model
2 implies a model with 2-way interactions, etc.

```{r}
summary(boost.boston)
```
We see that lstat and rm are by far the most important variables (again)

Produce partial dependence plots for these two variables (lstat and rm)
```{r}
par(mfrow=c(1,2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
par(mfrow=c(1,1))
```
These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables.
i.e., for every value of the selected variable, we calculate the predicted response for every combination of vallues of the other variables. We then average ("integrate out") over all these predicted responses. We do that for each value of the selected variable, which gives the graph.
As we might expect, median house prices are increasing with rm and decreasing with lstat.

Now use the boosted model to predict medv on the test set. Report the MSE
```{r}
yhat.boost=predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

The test MSE is 18.847, similar to random forest but lower than bagging.

What happens if we vary the shrinkage parameter from its default of 0.001 to 0.2? Report the test MSE.
```{r}
boost.boston = gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost=predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
```
In this case, using lambda = 0.2 leads to a slightly lower test MSE than lambda = 0.001.
